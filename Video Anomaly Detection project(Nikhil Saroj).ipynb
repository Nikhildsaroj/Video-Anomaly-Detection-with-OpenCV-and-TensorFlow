{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1f0PFVZt9X8RaAXgwRW1IEOxDYI99teHH","authorship_tag":"ABX9TyOAk7Yd0yKijz9lTLv0dkff"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"m4TI_lYlcuz4"}},{"cell_type":"markdown","source":["# **Video Anomaly Detection with OpenCV and TensorFlow**"],"metadata":{"id":"PSy44_Nbcuub"}},{"cell_type":"markdown","source":["# **Install Library**"],"metadata":{"id":"A4whfLfaFbXc"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"M7O-hNRUE-uy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710866719315,"user_tz":-330,"elapsed":22262,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"93149a67-5ec6-4d35-b655-498b62b1228c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Requirement already satisfied: object_detection in /usr/local/lib/python3.10/dist-packages (0.0.3)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from object_detection) (2.15.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object_detection) (3.0.9)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object_detection) (21.6.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object_detection) (9.4.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object_detection) (4.9.4)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (from object_detection) (1.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object_detection) (3.7.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->object_detection) (6.5.5)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter->object_detection) (5.5.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->object_detection) (6.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->object_detection) (6.5.4)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->object_detection) (5.5.6)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->object_detection) (7.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection) (2.8.2)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (24.3.7)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (1.62.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->object_detection) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->object_detection) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->object_detection) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->object_detection) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->object_detection) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->object_detection) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->object_detection) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->object_detection) (3.0.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->object_detection) (0.2.0)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->object_detection) (7.34.0)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->object_detection) (5.7.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->object_detection) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->object_detection) (6.3.3)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->object_detection) (3.6.6)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->object_detection) (3.0.10)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->object_detection) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->object_detection) (2.16.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (0.4)\n","Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (3.1.3)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (5.7.2)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (0.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (2.1.5)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (0.10.0)\n","Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (5.10.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->object_detection) (1.2.1)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (23.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (23.1.0)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (1.8.2)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (0.20.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->object_detection) (1.0.0)\n","Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter->object_detection) (2.4.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->object_detection) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->object_detection) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->object_detection) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->object_detection) (1.4.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->object_detection) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->object_detection) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->object_detection) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->object_detection) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->object_detection) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->object_detection) (4.9.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->object_detection) (4.2.0)\n","Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->object_detection) (1.24.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->object_detection) (0.2.4)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->object_detection) (2.19.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->object_detection) (4.19.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->object_detection) (0.2.13)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->object_detection) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->object_detection) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->object_detection) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->object_detection) (2024.2.2)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->object_detection) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->object_detection) (21.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->object_detection) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->object_detection) (0.5.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->object_detection) (0.8.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->object_detection) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->object_detection) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->object_detection) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->object_detection) (0.18.0)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->object_detection) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->object_detection) (1.7.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->object_detection) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->object_detection) (3.2.2)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->object_detection) (1.16.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->object_detection) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->object_detection) (1.2.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->object_detection) (2.21)\n"]}],"source":["!pip install opencv-python-headless\n","!pip install tensorflow\n","!pip install object_detection"]},{"cell_type":"markdown","source":[],"metadata":{"id":"UYn-AguUctLL"}},{"cell_type":"markdown","source":[],"metadata":{"id":"67S23zPFFY_Y"}},{"cell_type":"code","source":[],"metadata":{"id":"fTW9XVBhFaG9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Import Library**"],"metadata":{"id":"5tkFI73VJSsw"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","from object_detection.utils import visualization_utils as vis_util\n"],"metadata":{"id":"CFkSDXxQW2Z6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Load Pre Trained Model Faster R-CNN Inception v2**"],"metadata":{"id":"lCn7v26TFr5l"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def load_tf_model(model_path):\n","    model = tf.saved_model.load(model_path)\n","    return model\n","\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/faster_rcnn_inception_v2_coco_2018_01_28/faster_rcnn_inception_v2_coco_2018_01_28/saved_model\"\n","model = load_tf_model(model_path)"],"metadata":{"id":"hVeikJIIW2cj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","\n","# Function to load TensorFlow model\n","def load_tf_model(model_path):\n","    model = tf.saved_model.load(model_path)\n","    return model\n","\n","# Load TensorFlow model\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/faster_rcnn_inception_v2_coco_2018_01_28/faster_rcnn_inception_v2_coco_2018_01_28/saved_model\"\n","model = load_tf_model(model_path)\n","\n","# Path to the video file\n","video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","\n","# Open the video capture\n","cap = cv2.VideoCapture(video_path)\n","\n","# Loop over all frames in the video stream\n","while cap.isOpened():\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Preprocess the frame if necessary (e.g., resize, convert to RGB)\n","    # Perform object detection on the frame\n","    # Draw bounding boxes around detected objects\n","\n","    # Display the frame with bounding boxes\n","    cv2_imshow(frame)\n","\n","    # Check for key press to exit\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and close all OpenCV windows\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BbPEIMrcfT-ynnTZ7-zecDRGnf3vvi55"},"id":"gYhTBBS4heoF","executionInfo":{"status":"error","timestamp":1710848639355,"user_tz":-330,"elapsed":32943,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"55f14e66-db44-47b7-ba5a-2658bad95b14"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_yxVBoWUPSmm","executionInfo":{"status":"ok","timestamp":1710860680216,"user_tz":-330,"elapsed":65349,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"603aaf67-c60b-4f37-d900-f4d250fd89d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **Find Total Number of box detected and there Index**\n","\n","\n"],"metadata":{"id":"O7TI8OtJGMvK"}},{"cell_type":"code","source":["\"\"\"\n"," This script performs object detection using a TensorFlow model (Faster R-CNN Inception v2) on each frame of a video stream.\n","It detects objects with confidence scores greater than 0.5 and draws bounding boxes around them.\n"," The total number of detected objects and the frames where objects are detected are displayed.\n","\"\"\"\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","\n","# Function to load TensorFlow model\n","def load_tf_model(model_path):\n","    model = tf.saved_model.load(model_path)\n","    return model\n","\n","# Function to perform object detection on a frame\n","def perform_object_detection(frame, model, frame_number):\n","    # Preprocess the frame if necessary\n","    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    input_tensor = tf.convert_to_tensor(rgb_frame)\n","    input_tensor = input_tensor[tf.newaxis, ...]\n","\n","    # Perform object detection\n","    detections = model.signatures[\"serving_default\"](input_tensor)\n","\n","    # Extract bounding boxes from detections\n","    boxes = detections['detection_boxes'][0].numpy()\n","    scores = detections['detection_scores'][0].numpy()\n","    classes = detections['detection_classes'][0].numpy().astype(np.int32)\n","\n","    # Draw bounding boxes on the frame\n","    num_boxes = 0\n","    for i in range(len(boxes)):\n","        if scores[i] > 0.5:  # Filter out low-confidence detections\n","            num_boxes += 1\n","            ymin, xmin, ymax, xmax = boxes[i]\n","            xmin = int(xmin * frame.shape[1])\n","            xmax = int(xmax * frame.shape[1])\n","            ymin = int(ymin * frame.shape[0])\n","            ymax = int(ymax * frame.shape[0])\n","            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n","            cv2.putText(frame, f'Object {classes[i]}: {scores[i]}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","    return frame, num_boxes, frame_number\n","\n","\n","# Load the TensorFlow model\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/faster_rcnn_inception_v2_coco_2018_01_28/faster_rcnn_inception_v2_coco_2018_01_28/saved_model\"  # Replace with the path to your TensorFlow model\n","model = load_tf_model(model_path)\n","\n","# Path to the video file\n","video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","\n","# Open the video capture\n","cap = cv2.VideoCapture(video_path)\n","\n","# Loop over all frames in the video stream\n","total_boxes_detected = 0\n","detected_frames = []\n","frame_number = 0\n","while cap.isOpened():\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    frame_number += 1\n","    if not ret:\n","        break\n","\n","    # Perform object detection on the frame\n","    frame_with_boxes, num_boxes, frame_number = perform_object_detection(frame, model, frame_number)\n","\n","    # Accumulate the total number of boxes detected\n","    total_boxes_detected += num_boxes\n","\n","    # Display the frame with bounding boxes\n","    cv2_imshow(frame_with_boxes)\n","\n","    # Check if boxes are detected in this frame and add frame number to the list\n","    if num_boxes > 0:\n","        detected_frames.append(frame_number)\n","\n","    # Check for key press to exit\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and close all OpenCV windows\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# Print the total number of boxes detected\n","print(\"Total number of boxes detected:\", total_boxes_detected)\n","\n","# Print the frame numbers where boxes are detected\n","print(\"Frames with boxes detected:\", detected_frames)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133939,"output_embedded_package_id":"1iH103oDboCr5wRl0FUx2m7hBnj0ApbvM"},"id":"Nw90-1bFqRJN","executionInfo":{"status":"ok","timestamp":1710834320285,"user_tz":-330,"elapsed":172776,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"15bd2d86-40ae-4f82-f33e-a07d64f29954"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# **Finding the Anomaly Scores**"],"metadata":{"id":"LB5ZK1SCGY6C"}},{"cell_type":"code","source":["# Total number of frames\n","total_frames = 572\n","\n","# Frames with boxes detected\n","frames_with_boxes_detected = [257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 287, 288, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 311, 312, 313, 314, 319, 320, 331, 332, 333, 334, 335, 336, 337, 338, 341, 342, 347, 348, 349, 350, 351, 352]\n","\n","# Initialize anomaly scores with zeros for all frames\n","anomaly_scores = [0] * total_frames\n","\n","# Mark frames with boxes detected as anomalies\n","for frame_num in frames_with_boxes_detected:\n","    anomaly_scores[frame_num] = 1\n","\n","# Print the total number of frames and the anomaly scores\n","print(\"Total frames:\", total_frames)\n","print(\"Anomaly Scores:\", anomaly_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ht3SWhECrqgi","executionInfo":{"status":"ok","timestamp":1710834520246,"user_tz":-330,"elapsed":11,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"1e65da31-e2e8-4ebf-f2f9-437da77b74df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total frames: 572\n","Anomaly Scores: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"markdown","source":["# **Ploting & Finding Anomaly detected at index**"],"metadata":{"id":"myKlUgMkGlSe"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Anomaly scores\n","anomaly_scores = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","# Analyze Anomaly Score Distribution\n","plt.hist(anomaly_scores, bins=10)\n","plt.xlabel('Anomaly Score')\n","plt.ylabel('Frequency')\n","plt.title('Anomaly Score Distribution')\n","plt.show()\n","\n","# Define Thresholds for Anomaly Scores\n","normal_threshold = 0.6\n","anomaly_threshold = 0.8\n","\n","# Trigger Alerts for Anomalies\n","for i, score in enumerate(anomaly_scores):\n","    if score >= anomaly_threshold:\n","        print(f\"Anomaly detected at index {i}, with score: {score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1424},"id":"9owrzpI7xOey","executionInfo":{"status":"ok","timestamp":1710835957104,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"fceada1d-c0da-40f1-cb9c-4273dab406e2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Anomaly detected at index 299, with score: 1\n","Anomaly detected at index 300, with score: 1\n","Anomaly detected at index 301, with score: 1\n","Anomaly detected at index 302, with score: 1\n","Anomaly detected at index 303, with score: 1\n","Anomaly detected at index 304, with score: 1\n","Anomaly detected at index 305, with score: 1\n","Anomaly detected at index 306, with score: 1\n","Anomaly detected at index 307, with score: 1\n","Anomaly detected at index 308, with score: 1\n","Anomaly detected at index 309, with score: 1\n","Anomaly detected at index 310, with score: 1\n","Anomaly detected at index 315, with score: 1\n","Anomaly detected at index 316, with score: 1\n","Anomaly detected at index 317, with score: 1\n","Anomaly detected at index 318, with score: 1\n","Anomaly detected at index 319, with score: 1\n","Anomaly detected at index 320, with score: 1\n","Anomaly detected at index 321, with score: 1\n","Anomaly detected at index 322, with score: 1\n","Anomaly detected at index 323, with score: 1\n","Anomaly detected at index 324, with score: 1\n","Anomaly detected at index 325, with score: 1\n","Anomaly detected at index 326, with score: 1\n","Anomaly detected at index 329, with score: 1\n","Anomaly detected at index 330, with score: 1\n","Anomaly detected at index 337, with score: 1\n","Anomaly detected at index 338, with score: 1\n","Anomaly detected at index 339, with score: 1\n","Anomaly detected at index 340, with score: 1\n","Anomaly detected at index 341, with score: 1\n","Anomaly detected at index 342, with score: 1\n","Anomaly detected at index 343, with score: 1\n","Anomaly detected at index 344, with score: 1\n","Anomaly detected at index 345, with score: 1\n","Anomaly detected at index 346, with score: 1\n","Anomaly detected at index 347, with score: 1\n","Anomaly detected at index 348, with score: 1\n","Anomaly detected at index 349, with score: 1\n","Anomaly detected at index 350, with score: 1\n","Anomaly detected at index 353, with score: 1\n","Anomaly detected at index 354, with score: 1\n","Anomaly detected at index 355, with score: 1\n","Anomaly detected at index 356, with score: 1\n","Anomaly detected at index 361, with score: 1\n","Anomaly detected at index 362, with score: 1\n","Anomaly detected at index 373, with score: 1\n","Anomaly detected at index 374, with score: 1\n","Anomaly detected at index 375, with score: 1\n","Anomaly detected at index 376, with score: 1\n","Anomaly detected at index 377, with score: 1\n","Anomaly detected at index 378, with score: 1\n","Anomaly detected at index 379, with score: 1\n","Anomaly detected at index 380, with score: 1\n","Anomaly detected at index 383, with score: 1\n","Anomaly detected at index 384, with score: 1\n","Anomaly detected at index 389, with score: 1\n","Anomaly detected at index 390, with score: 1\n","Anomaly detected at index 391, with score: 1\n","Anomaly detected at index 392, with score: 1\n","Anomaly detected at index 393, with score: 1\n","Anomaly detected at index 394, with score: 1\n"]}]},{"cell_type":"markdown","source":["# **Detect the Object in the video**"],"metadata":{"id":"s_FsqopbG9fY"}},{"cell_type":"code","source":["import cv2\n","import tensorflow as tf\n","from PIL import Image\n","from IPython.display import display\n","\n","class VideoApp:\n","    def __init__(self, video_source=0, model_path=None):\n","        self.video_source = video_source\n","        self.vid = cv2.VideoCapture(self.video_source)\n","\n","        # Load TensorFlow model for object detection\n","        if model_path:\n","            self.model = tf.saved_model.load(model_path)\n","        else:\n","            self.model = None\n","\n","        self.update()\n","\n","    def update(self):\n","        while self.vid.isOpened():  # Keep reading frames until no more frames are available\n","            ret, frame = self.vid.read()\n","            if not ret:\n","                break\n","\n","            # Process frame\n","            processed_frame, anomaly_detected = self.process_frame(frame)\n","\n","            # Save processed frame as image\n","            cv2.imwrite('processed_frame.jpg', processed_frame)\n","\n","            # Display processed frame with anomaly indicator\n","            if anomaly_detected:\n","                display(Image.open('processed_frame.jpg').resize((640, 480)))\n","            else:\n","                display(Image.open('processed_frame.jpg'))\n","\n","        self.vid.release()\n","\n","    def process_frame(self, frame):\n","        # Placeholder for processing frame\n","        # Perform object detection and anomaly detection\n","\n","        anomaly_detected = False\n","\n","        # Example: Object detection\n","        if self.model:\n","            # Preprocess frame for object detection\n","            input_tensor = tf.convert_to_tensor(frame)\n","            input_tensor = input_tensor[tf.newaxis, ...]\n","\n","            # Perform inference\n","            detections = self.model.signatures[\"serving_default\"](input_tensor)\n","\n","            # Extract information from detections and draw bounding boxes\n","            # Adjust this part according to the structure of your loaded model\n","            for i in range(detections['detection_boxes'].shape[1]):\n","                confidence = detections['detection_scores'][0, i].numpy()\n","                if confidence > 0.5:  # Confidence threshold\n","                    box = detections['detection_boxes'][0, i].numpy()\n","                    ymin, xmin, ymax, xmax = box\n","                    h, w, _ = frame.shape\n","                    x1, y1, x2, y2 = int(xmin * w), int(ymin * h), int(xmax * w), int(ymax * h)\n","                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","                    cv2.putText(frame, 'Object Detected', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","        # Example: Anomaly detection\n","        # Placeholder code for anomaly detection based on object detection results\n","        if detections['detection_boxes'].shape[1] > 0:\n","            anomaly_detected = True\n","\n","        return frame, anomaly_detected\n","\n","# Run the application\n","app = VideoApp(video_source=\"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\",\n","               model_path=\"/content/drive/MyDrive/Colab Notebooks/save AI models/faster_rcnn_inception_v2_coco_2018_01_28/faster_rcnn_inception_v2_coco_2018_01_28/saved_model\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178338,"output_embedded_package_id":"1aXqKr-2LWDPxXDLfhu2eIXO4g_oD9ZCO"},"id":"wcD-IlNz_DGi","executionInfo":{"status":"ok","timestamp":1710841288865,"user_tz":-330,"elapsed":1040661,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"cffa587b-877b-4b1b-f5d2-fb0010f38113"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# **Object Detection in Video using TensorFlow and OpenCV with Values**"],"metadata":{"id":"zSfp9frWHnb5"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","\n","# Function to load TensorFlow model\n","def load_tf_model(model_path):\n","    model = tf.saved_model.load(model_path)\n","    return model\n","\n","# Function to perform object detection on a frame\n","def perform_object_detection(frame, model):\n","    # Preprocess the frame if necessary\n","    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    input_tensor = tf.convert_to_tensor(rgb_frame)\n","    input_tensor = input_tensor[tf.newaxis, ...]\n","\n","    # Perform object detection\n","    detections = model.signatures[\"serving_default\"](input_tensor)\n","\n","    # Extract bounding boxes from detections\n","    boxes = detections['detection_boxes'][0].numpy()\n","    scores = detections['detection_scores'][0].numpy()\n","    classes = detections['detection_classes'][0].numpy().astype(np.int32)\n","\n","    # Draw bounding boxes on the frame\n","    for i in range(len(boxes)):\n","        if scores[i] > 0.5:  # Filter out low-confidence detections\n","            ymin, xmin, ymax, xmax = boxes[i]\n","            xmin = int(xmin * frame.shape[1])\n","            xmax = int(xmax * frame.shape[1])\n","            ymin = int(ymin * frame.shape[0])\n","            ymax = int(ymax * frame.shape[0])\n","            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n","            cv2.putText(frame, f'Object {classes[i]}: {scores[i]}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","    return frame\n","\n","\n","\n","# Load the TensorFlow model\n","model = load_tf_model(model_path)\n","\n","# Path to the video file\n","video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","\n","# Open the video capture\n","cap = cv2.VideoCapture(video_path)\n","\n","# Loop over all frames in the video stream\n","while cap.isOpened():\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Perform object detection on the frame\n","    frame_with_boxes = perform_object_detection(frame, model)\n","\n","    # Display the frame with bounding boxes\n","    cv2_imshow(frame_with_boxes)\n","\n","    # Check for key press to exit\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and close all OpenCV windows\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1rUa8d3-PVWxtrjWlaugfycOcQZb1GoYd"},"id":"b6r1rR_nW2fB","executionInfo":{"status":"ok","timestamp":1710795994881,"user_tz":-330,"elapsed":198254,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"5d00b15a-9a55-4d23-8421-2bb0e81d20fd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# **Background Subtraction in Video Stream**"],"metadata":{"id":"eSs_ZgjKL3lM"}},{"cell_type":"code","source":["\"\"\"\n","This script performs background subtraction on each frame of a video stream using the\n","Gaussian Mixture-based Background/Foreground Segmentation method (MOG2).\n","It subtracts the background from the current frame to highlight moving objects.\n","The resulting foreground mask is displayed in real-time using OpenCV's imshow function.\n","\"\"\"\n","\n","from google.colab.patches import cv2_imshow\n","import cv2\n","\n","# Open the video capture\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\")\n","\n","# Initialize background subtractor\n","fgbg = cv2.createBackgroundSubtractorMOG2()\n","\n","# Loop over all frames in the video stream\n","while cap.isOpened():\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Apply background subtraction\n","    fgmask = fgbg.apply(frame)\n","\n","    # Display the background subtracted frame\n","    cv2_imshow(fgmask)\n","\n","    # Check for key press to exit\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture\n","cap.release()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133758,"output_embedded_package_id":"1E08pEg4gEHtnxAING_4pg6AkgJxnouJn"},"id":"Rp90A0BWK8XM","executionInfo":{"status":"ok","timestamp":1710842751081,"user_tz":-330,"elapsed":33367,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"93d9a4f6-6cb7-456c-9f67-531429549173"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# **Number of Anomaly Detection in Shark Video using Optical Flow**"],"metadata":{"id":"3u7iR9mWIZQ0"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Function to compute anomaly scores for player movements\n","def calculate_player_movement_anomaly_scores(frames):\n","    anomaly_scores = []\n","    prev_frame_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n","\n","    for frame in frames[1:]:\n","        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Compute optical flow between consecutive frames\n","        flow = cv2.calcOpticalFlowFarneback(prev_frame_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","\n","        # Calculate magnitude of optical flow vectors\n","        magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n","\n","        # Compute anomaly score based on average flow magnitude\n","        anomaly_score = np.mean(magnitude)\n","        anomaly_scores.append(anomaly_score)\n","\n","        prev_frame_gray = frame_gray\n","\n","    return anomaly_scores\n","\n","# Function to detect anomalies based on threshold\n","def detect_anomalies(anomaly_scores, threshold):\n","    anomalies = [score > threshold for score in anomaly_scores]\n","    return anomalies\n","\n","# Load basketball game video clip\n","video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","cap = cv2.VideoCapture(video_path)\n","\n","if not cap.isOpened():\n","    print(\"Error: Unable to open video file.\")\n","    exit()\n","\n","# Read frames from video clip\n","frames = []\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    frames.append(frame)\n","\n","cap.release()\n","\n","# Calculate anomaly scores for player movements\n","player_movement_anomaly_scores = calculate_player_movement_anomaly_scores(frames)\n","\n","# Set anomaly detection threshold\n","threshold = 0.1\n","\n","# Detect anomalies based on threshold\n","anomalies = detect_anomalies(player_movement_anomaly_scores, threshold)\n","\n","# Print the total number of frames and the number of detected anomalies\n","print(\"Total frames:\", len(frames))\n","print(\"Number of anomalies detected:\", sum(anomalies))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXbzcfeUW2hd","executionInfo":{"status":"ok","timestamp":1710796150419,"user_tz":-330,"elapsed":43805,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"aba6c2e1-c0e1-4d6d-d4d8-f5a760be163a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total frames: 572\n","Number of anomalies detected: 287\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TUvZer47NKLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yPsnGTcHmiGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Finding the Restricted Area in Video Stream**"],"metadata":{"id":"5klbuzmNI6vt"}},{"cell_type":"code","source":["\n","\"\"\"\n","Interactive Selection of Restricted Area in Video Stream\n","\n","This script allows users to interactively select a restricted area in a video stream by clicking on four points.\n","The selected points define the coordinates and dimensions of the restricted area.\n","\n","\"\"\"\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","# Function to get points from mouse click event\n","def get_points(event):\n","    if event.button == 1:\n","        x = int(event.xdata)\n","        y = int(event.ydata)\n","        points.append((x, y))\n","        plt.scatter(x, y, c='g', marker='o')\n","        plt.draw()\n","\n","# Path to the video file\n","video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","\n","# Open the video capture\n","cap = cv2.VideoCapture(video_path)\n","if not cap.isOpened():\n","    print(\"Error: Unable to open video file.\")\n","    exit()\n","\n","# List to store selected points\n","points = []\n","\n","# Add predefined points\n","predefined_points = [(100, 100), (100, 500), (300, 100), (300, 500)]\n","for point in predefined_points:\n","    points.append(point)\n","    plt.scatter(point[0], point[1], c='g', marker='o')\n","\n","# Create a plot to display the video and points\n","plt.figure(figsize=(10, 6))\n","plt.title(\"Select 4 points to define the restricted area (click to select)\")\n","\n","# Set mouse event handler\n","plt.connect('button_press_event', get_points)\n","\n","# Loop over all frames in the video\n","while True:\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Display the frame using matplotlib\n","    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","    plt.pause(0.001)  # Pause to update the plot\n","\n","# Close the video capture\n","cap.release()\n","\n","# Calculate coordinates and dimensions of the restricted area\n","if len(points) == 4:\n","    x = min(points[0][0], points[1][0], points[2][0], points[3][0])\n","    y = min(points[0][1], points[1][1], points[2][1], points[3][1])\n","    width = max(points[0][0], points[1][0], points[2][0], points[3][0]) - x\n","    height = max(points[0][1], points[1][1], points[2][1], points[3][1]) - y\n","    print(\"Restricted area coordinates (x, y):\", x, y)\n","    print(\"Restricted area dimensions (width, height):\", width, height)\n","else:\n","    print(\"Error: Four points are required to define the restricted area.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141332,"output_embedded_package_id":"1jeOFIdHOK_iSZgpcK-AW-NlEwitZTY_I"},"id":"6l2LHX7nW2o7","executionInfo":{"status":"ok","timestamp":1710831313939,"user_tz":-330,"elapsed":248139,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"d5a0a9a7-4514-4222-d1dc-ad6c05866e7c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# **Final Testing Object Detection and Anomaly Detection on Shark Video Stream**"],"metadata":{"id":"MiS_6jOqVewQ"}},{"cell_type":"code","source":["\n","\"\"\"\n","This section focuses on testing the object detection and anomaly detection functionalities\n","on a shark video stream. The code utilizes a TensorFlow model (Faster R-CNN Inception v2)\n","to detect objects with confidence scores above 0.5 and marks frames where sharks are detected as anomalies.\n","The testing process involves loading the TensorFlow model, reading frames from the video stream,\n"," performing object detection, displaying the video stream with bounding boxes and anomaly indicators,\n","and finally printing whether any anomalies, specifically shark detections, are detected.\n","\n","\"\"\"\n","\n","\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","\n","# Function to load TensorFlow model\n","def load_tf_model(model_path):\n","    model = tf.saved_model.load(model_path)\n","    return model\n","\n","# Function to perform object detection on a frame\n","def perform_object_detection(frame, model):\n","    # Preprocess the frame if necessary\n","    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    input_tensor = tf.convert_to_tensor(rgb_frame)\n","    input_tensor = input_tensor[tf.newaxis, ...]\n","\n","    # Perform object detection\n","    detections = model.signatures[\"serving_default\"](input_tensor)\n","\n","    # Extract bounding boxes from detections\n","    boxes = detections['detection_boxes'][0].numpy()\n","    scores = detections['detection_scores'][0].numpy()\n","    classes = detections['detection_classes'][0].numpy().astype(np.int32)\n","\n","    # Filter out low-confidence detections\n","    threshold = 0.5\n","    detected_boxes = []\n","    for i in range(len(boxes)):\n","        if scores[i] > threshold:\n","            ymin, xmin, ymax, xmax = boxes[i]\n","            height, width, _ = frame.shape\n","            x_min = int(xmin * width)\n","            y_min = int(ymin * height)\n","            x_max = int(xmax * width)\n","            y_max = int(ymax * height)\n","            detected_boxes.append((x_min, y_min, x_max, y_max))\n","\n","    # Return whether a shark is detected or not\n","    return len(detected_boxes) > 0, detected_boxes\n","\n","# Function to display the video stream with bounding boxes and anomaly indicators\n","def display_video_stream(frame, shark_detected):\n","    # Draw message if a shark is detected\n","    if shark_detected:\n","        cv2.putText(frame, 'A shark detected', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","    # Display the frame\n","    cv2_imshow(frame)\n","\n","# Load the TensorFlow model\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/faster_rcnn_inception_v2_coco_2018_01_28/faster_rcnn_inception_v2_coco_2018_01_28/saved_model\"\n","model = load_tf_model(model_path)\n","\n","# Path to the video file\n","video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","\n","# Open the video capture\n","cap = cv2.VideoCapture(video_path)\n","\n","# Variables to track anomaly detection\n","anomaly_detected = False\n","anomaly_type = None\n","\n","# Loop over all frames in the video stream\n","while cap.isOpened():\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Perform object detection on the frame\n","    shark_detected, shark_boxes = perform_object_detection(frame, model)\n","\n","    # Display the video stream with bounding boxes and anomaly indicators\n","    display_video_stream(frame, shark_detected)\n","\n","    # Check if anomaly is detected\n","    if shark_detected:\n","        anomaly_detected = True\n","        anomaly_type = \"Shark\"\n","\n","    # Check for key press to exit\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and close all OpenCV windows\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# Print the result of anomaly detection\n","if anomaly_detected:\n","    print(\"Anomaly Detected:\", anomaly_type)\n","else:\n","    print(\"No Anomaly Detected\")\n"],"metadata":{"id":"x20jIfYtW2ra","executionInfo":{"status":"ok","timestamp":1710845319171,"user_tz":-330,"elapsed":998389,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"colab":{"base_uri":"https://localhost:8080/","height":133775,"output_embedded_package_id":"1mYCliDxu3Nkp2f6yfV2-orwjwKNkr5du"},"outputId":"7b5d1ad3-ad3e-4e8e-f1c2-9a5d95849a8a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"_t-RRObHsddT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZvL2Vzzud0d7","executionInfo":{"status":"ok","timestamp":1710869574879,"user_tz":-330,"elapsed":416,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# **Shark Detection in Video Stream using TensorFlow and OpenCV and to Save the Video**"],"metadata":{"id":"psF7cPvKxAoY"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","\n","# Function to load TensorFlow model\n","def load_tf_model(model_path):\n","    model = tf.saved_model.load(model_path)\n","    return model\n","\n","# Function to perform object detection on a frame\n","def perform_object_detection(frame, model):\n","    # Preprocess the frame if necessary\n","    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    input_tensor = tf.convert_to_tensor(rgb_frame)\n","    input_tensor = input_tensor[tf.newaxis, ...]\n","\n","    # Perform object detection\n","    detections = model.signatures[\"serving_default\"](input_tensor)\n","\n","    # Extract bounding boxes from detections\n","    boxes = detections['detection_boxes'][0].numpy()\n","    scores = detections['detection_scores'][0].numpy()\n","    classes = detections['detection_classes'][0].numpy().astype(np.int32)\n","\n","    # Filter out low-confidence detections\n","    threshold = 0.5\n","    detected_boxes = []\n","    for i in range(len(boxes)):\n","        if scores[i] > threshold:\n","            ymin, xmin, ymax, xmax = boxes[i]\n","            height, width, _ = frame.shape\n","            x_min = int(xmin * width)\n","            y_min = int(ymin * height)\n","            x_max = int(xmax * width)\n","            y_max = int(ymax * height)\n","            detected_boxes.append((x_min, y_min, x_max, y_max))\n","\n","            # Draw bounding boxes on the frame\n","            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n","\n","    return frame, len(detected_boxes) > 0\n","\n","# Load the TensorFlow model\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/faster_rcnn_inception_v2_coco_2018_01_28/faster_rcnn_inception_v2_coco_2018_01_28/saved_model\"\n","model = load_tf_model(model_path)\n","\n","# Path to the input video file\n","input_video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/shark1.mp4\"\n","\n","# Path to the output video file with shark detection\n","output_video_path = \"/content/drive/MyDrive/Colab Notebooks/save AI models/sharkdetection.avi\"\n","\n","# Open the input video capture\n","cap = cv2.VideoCapture(input_video_path)\n","\n","# Get the video properties\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# Define the codec and create VideoWriter object\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","# Loop over all frames in the input video stream\n","while cap.isOpened():\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Perform object detection on the frame\n","    frame_with_boxes, shark_detected = perform_object_detection(frame, model)\n","\n","    # Display the frame with bounding boxes\n","    if shark_detected:\n","        cv2.putText(frame_with_boxes, 'A shark detected', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","    cv2_imshow(frame_with_boxes)\n","\n","    # Write the processed frame to the output video\n","    out.write(frame_with_boxes)\n","\n","    # Check for key press to exit\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and close all OpenCV windows\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()\n","\n","print(\"Shark detection video saved successfully!\")\n"],"metadata":{"id":"Gi2CrlDsnURz","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1jUT7bgv7KVy9IwzRl-preOwJ2tAzeQVl"},"executionInfo":{"status":"ok","timestamp":1710869266562,"user_tz":-330,"elapsed":984048,"user":{"displayName":"Nikhil Saroj","userId":"01288406837805717615"}},"outputId":"6a0eb9c8-268d-494d-ef5a-331eb5e39e86"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"D-RKgZu-W2t9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pjTuGtM1W2wT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8g59ZuPtW2x_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KP6hld83lfXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r1tMStTUWy0h"},"execution_count":null,"outputs":[]}]}